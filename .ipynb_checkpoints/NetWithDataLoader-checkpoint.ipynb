{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "npGiuvftxf1z",
    "outputId": "48a8cb76-f300-4db3-f216-eba77d4cce90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.18</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import itertools\n",
    "import pixiedust\n",
    "import random\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "plt.ion()   # interactive mode\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "MHoa2aySwBF_",
    "outputId": "31d7f712-1312-4029-8773-9d4b77d44312"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    LABS_DIR = Path ('/content/gdrive/My Drive/Labs')\n",
    "except:\n",
    "    LABS_DIR = Path ('C:/Labs/')\n",
    "\n",
    "DATA_DIR = LABS_DIR/'Data'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRBBa4e-xRBD"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# for validatin we use normalization and resize (for train we also change the angle and size of the images)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "hmElu6Slwjgf",
    "outputId": "6b438b4a-4913-46c6-9bc5-6884fcfa2d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
      "Train image size: 100\n",
      "Validation image size: 100\n"
     ]
    }
   ],
   "source": [
    "''' The function takes the data loader and a parameter  '''\n",
    "def create_train_val_slice(data,train_limit=None,val_same_as_train=False, val_limit=None):\n",
    "    train1, train2 = itertools.tee(islice(data['train'],train_limit))\n",
    "    result_slice = {}\n",
    "    result_slice['train'] = train1\n",
    "    if val_same_as_train:\n",
    "        result_slice['val'] = train2\n",
    "    else:\n",
    "        result_slice['val'] = islice(data['val'],val_limit)\n",
    "    return result_slice\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "#                                               shuffle=True, num_workers=1)\n",
    "#               for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "sample_size = 100\n",
    "sample_n = {x: random.sample(list(range(dataset_sizes[x])), sample_size)\n",
    "            for x in ['train', 'val']}\n",
    "\n",
    "image_datasets_reduced = {x: torch.utils.data.Subset(image_datasets['train'], sample_n['train'])\n",
    "                          for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets_reduced[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders_reduced = {x: torch.utils.data.DataLoader(image_datasets_reduced[x], batch_size=8,\n",
    "                                              shuffle=True, num_workers=1)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Classes: \", class_names) \n",
    "print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
    "print(f'Validation image size: {dataset_sizes[\"val\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "aOMEb5deCofk",
    "outputId": "40524968-328f-420d-cfc1-1e3fe2632a01"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "# # Make a grid from batch\n",
    "# sample_train_images = torchvision.utils.make_grid(inputs)\n",
    "# #imshow(sample_train_images, title=classes)\n",
    "# print(f\"classes={classes}\")\n",
    "# imshow(sample_train_images, title=[class_names[i] for i in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     25,
     27,
     79
    ],
    "colab": {},
    "colab_type": "code",
    "id": "sDNvKgitDNG7",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data, model, criterion, optimizer, scheduler, num_epochs=2, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print('Val loss: {}, Val accuracy: {}'\\\n",
    "              .format(checkpoint[\"best_val_loss\"], checkpoint[\"best_val_accuracy\"]))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "           \n",
    "    print(\"Starting epochs\")\n",
    "    outer = tqdm(total=num_epochs, desc='Epoch', position=0)\n",
    "    inner = tqdm(total=500, position=1)\n",
    "    for epoch in range(num_epochs):\n",
    "        outer.update(1)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            total = 500\n",
    "\n",
    "            # Handle tqdm inner loop counter\n",
    "            inner.total = dataset_sizes[phase]\n",
    "            inner.reset()\n",
    "            inner_total = 0 \n",
    "            \n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data[phase]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i>0:\n",
    "                    report_str = '[%d, %d] loss: %.3f' % (epoch + 1, i, running_loss / i * inputs.size(0))\n",
    "                else:\n",
    "                    report_str = \"first iteration\"\n",
    "\n",
    "                # Update inner tqdm, we are about to override the previous maximum, update maximum\n",
    "                if inner_total >= total:\n",
    "                    total = total *2\n",
    "                    inner.total = total\n",
    "                inner_total = inner_total + 1\n",
    "                inner.update(1) # Advance the tqdm counter\n",
    "                inner.desc = f'Phase: {phase} ' + report_str\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                # print(\"running_corrects =\", running_corrects)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            #inner.write(\"running_corrects=\", running_corrects, \" epoch: \", epoch)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            loss_str = f'Epoch: {epoch+1} of {num_epochs}, {phase:6} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}'\n",
    "            inner.write(loss_str)\n",
    "            \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('New best model found!')\n",
    "                print('New record loss:{}, previous record loss: {}'.format(epoch_loss, best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                #save the weights\n",
    "                torch.save(best_model_wts, CHECK_POINT_PATH)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "3876f5ef3ff947b7b26c72b6512166f8",
      "79f9ecfbdd9a4871b95ae9668720c847",
      "fa2dc4675ff940b48a90425981eb55a6",
      "2e81eb8a8cdb40738fb85ce51cfed6c7",
      "966b7f8371654dd2a54f484c4a9c0129",
      "a315579ab89f43e2b7d184c7c4d64baa",
      "6d2c8e3068854041be63a92194b35a64",
      "08e0b965fc614feb960cf954223db3e3"
     ]
    },
    "colab_type": "code",
    "id": "AUF-cgM1y_7L",
    "outputId": "107315c6-103a-4192-a611-d3e76809d530"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to C:\\Users\\Razon/.cache\\torch\\checkpoints\\resnet101-5d3b4d8f.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe255dc437ca4d818fe38a6073f94dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "#model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "#model_conv = torchvision.models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_conv.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35uVilmuNgg-"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UgElP8XDrYm"
   },
   "outputs": [],
   "source": [
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "\n",
    "ct = 0\n",
    "for child in model_conv.children():\n",
    "  ct += 1\n",
    "  # freezes layers 1-6 in the total 10 layers of Resnet50\n",
    "  if ct < 7:\n",
    "    for param in child.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "'''two options to write the loss. They are both equal'''\n",
    "# option 1 #\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# option 2 #\n",
    "# p = nn.functional.softmax(model_conv, dim=1)\n",
    "# # to calculate loss using probabilities you can do below \n",
    "# criterion = nn.functional.nll_loss(torch.log(p), y)\n",
    "\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IaCCoou62TKD",
    "outputId": "058f59dd-7db0-4456-f29d-42ae2d4567c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint not found\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = LABS_DIR/'ModelParams'/'checkpoint.tar'\n",
    "\n",
    "!del $CHECK_POINT_PATH\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_tKaq9vU0cjq",
    "outputId": "1b3c5122-2b8f-4c1c-cedc-811cbfe8fcfe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265688cc91104e8d8e7dcea8b93ec5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', style=ProgressStyle(description_width='initial'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b0f80a19b44bcdb4f4109f474ea4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 of 100, train  Loss: 1.8601 Acc: 0.2600\n",
      "Epoch: 1 of 100, val    Loss: 1.8023 Acc: 0.1900\n",
      "New best model found!\n",
      "New record loss:1.8022623348236084, previous record loss: inf\n",
      "\n",
      "Epoch: 2 of 100, train  Loss: 1.7231 Acc: 0.3200\n",
      "Epoch: 2 of 100, val    Loss: 1.6900 Acc: 0.2900\n",
      "New best model found!\n",
      "New record loss:1.6899969863891602, previous record loss: 1.8022623348236084\n",
      "\n",
      "Epoch: 3 of 100, train  Loss: 1.6279 Acc: 0.3400\n",
      "Epoch: 3 of 100, val    Loss: 1.7338 Acc: 0.2100\n",
      "\n",
      "Epoch: 4 of 100, train  Loss: 1.6152 Acc: 0.3700\n",
      "Epoch: 4 of 100, val    Loss: 1.6900 Acc: 0.2300\n",
      "\n",
      "Epoch: 5 of 100, train  Loss: 1.5405 Acc: 0.3800\n",
      "Epoch: 5 of 100, val    Loss: 1.5364 Acc: 0.3500\n",
      "New best model found!\n",
      "New record loss:1.5364361000061035, previous record loss: 1.6899969863891602\n",
      "\n",
      "Epoch: 6 of 100, train  Loss: 1.6503 Acc: 0.2800\n",
      "Epoch: 6 of 100, val    Loss: 1.3420 Acc: 0.4400\n",
      "New best model found!\n",
      "New record loss:1.3420453214645385, previous record loss: 1.5364361000061035\n",
      "\n",
      "Epoch: 7 of 100, train  Loss: 1.4249 Acc: 0.3900\n",
      "Epoch: 7 of 100, val    Loss: 1.3433 Acc: 0.4700\n",
      "\n",
      "Epoch: 8 of 100, train  Loss: 1.3557 Acc: 0.4500\n",
      "Epoch: 8 of 100, val    Loss: 1.3095 Acc: 0.5800\n",
      "New best model found!\n",
      "New record loss:1.3094695758819581, previous record loss: 1.3420453214645385\n",
      "\n",
      "Epoch: 9 of 100, train  Loss: 1.3198 Acc: 0.5000\n",
      "Epoch: 9 of 100, val    Loss: 1.2797 Acc: 0.6100\n",
      "New best model found!\n",
      "New record loss:1.2796861553192138, previous record loss: 1.3094695758819581\n",
      "\n",
      "Epoch: 10 of 100, train  Loss: 1.3116 Acc: 0.4500\n",
      "Epoch: 10 of 100, val    Loss: 1.2954 Acc: 0.5600\n",
      "\n",
      "Epoch: 11 of 100, train  Loss: 1.3192 Acc: 0.5200\n",
      "Epoch: 11 of 100, val    Loss: 1.2193 Acc: 0.6400\n",
      "New best model found!\n",
      "New record loss:1.2193181347846984, previous record loss: 1.2796861553192138\n",
      "\n",
      "Epoch: 12 of 100, train  Loss: 1.3020 Acc: 0.5300\n",
      "Epoch: 12 of 100, val    Loss: 1.2233 Acc: 0.5800\n",
      "\n",
      "Epoch: 13 of 100, train  Loss: 1.2601 Acc: 0.5800\n",
      "Epoch: 13 of 100, val    Loss: 1.2135 Acc: 0.6200\n",
      "New best model found!\n",
      "New record loss:1.2134781169891358, previous record loss: 1.2193181347846984\n",
      "\n",
      "Epoch: 14 of 100, train  Loss: 1.3065 Acc: 0.5300\n",
      "Epoch: 14 of 100, val    Loss: 1.2626 Acc: 0.6000\n",
      "\n",
      "Epoch: 15 of 100, train  Loss: 1.3132 Acc: 0.5400\n",
      "Epoch: 15 of 100, val    Loss: 1.1874 Acc: 0.6000\n",
      "New best model found!\n",
      "New record loss:1.1874189376831055, previous record loss: 1.2134781169891358\n",
      "\n",
      "Epoch: 16 of 100, train  Loss: 1.2560 Acc: 0.5700\n",
      "Epoch: 16 of 100, val    Loss: 1.2559 Acc: 0.6300\n",
      "\n",
      "Epoch: 17 of 100, train  Loss: 1.2566 Acc: 0.5800\n",
      "Epoch: 17 of 100, val    Loss: 1.2228 Acc: 0.6100\n",
      "\n",
      "Epoch: 18 of 100, train  Loss: 1.2577 Acc: 0.5700\n",
      "Epoch: 18 of 100, val    Loss: 1.2110 Acc: 0.6800\n",
      "\n",
      "Epoch: 19 of 100, train  Loss: 1.2486 Acc: 0.6000\n",
      "Epoch: 19 of 100, val    Loss: 1.1578 Acc: 0.6800\n",
      "New best model found!\n",
      "New record loss:1.1578330898284912, previous record loss: 1.1874189376831055\n",
      "\n",
      "Epoch: 20 of 100, train  Loss: 1.2040 Acc: 0.6500\n",
      "Epoch: 20 of 100, val    Loss: 1.2222 Acc: 0.6100\n",
      "\n",
      "Epoch: 21 of 100, train  Loss: 1.2759 Acc: 0.5500\n",
      "Epoch: 21 of 100, val    Loss: 1.2310 Acc: 0.6100\n",
      "\n",
      "Epoch: 22 of 100, train  Loss: 1.2604 Acc: 0.5800\n",
      "Epoch: 22 of 100, val    Loss: 1.2071 Acc: 0.6500\n",
      "\n",
      "Epoch: 23 of 100, train  Loss: 1.2842 Acc: 0.5200\n",
      "Epoch: 23 of 100, val    Loss: 1.2007 Acc: 0.6800\n",
      "\n",
      "Epoch: 24 of 100, train  Loss: 1.2406 Acc: 0.5900\n",
      "Epoch: 24 of 100, val    Loss: 1.2371 Acc: 0.5800\n",
      "\n",
      "Epoch: 25 of 100, train  Loss: 1.2588 Acc: 0.6200\n",
      "Epoch: 25 of 100, val    Loss: 1.2520 Acc: 0.5300\n",
      "\n",
      "Epoch: 26 of 100, train  Loss: 1.2300 Acc: 0.5900\n",
      "Epoch: 26 of 100, val    Loss: 1.1802 Acc: 0.6800\n",
      "\n",
      "Epoch: 27 of 100, train  Loss: 1.2577 Acc: 0.5600\n",
      "Epoch: 27 of 100, val    Loss: 1.2028 Acc: 0.6300\n",
      "\n",
      "Epoch: 28 of 100, train  Loss: 1.2147 Acc: 0.6400\n",
      "Epoch: 28 of 100, val    Loss: 1.1821 Acc: 0.6500\n",
      "\n",
      "Epoch: 29 of 100, train  Loss: 1.2667 Acc: 0.5600\n",
      "Epoch: 29 of 100, val    Loss: 1.1748 Acc: 0.6100\n",
      "\n",
      "Epoch: 30 of 100, train  Loss: 1.2913 Acc: 0.5900\n",
      "Epoch: 30 of 100, val    Loss: 1.2344 Acc: 0.6800\n",
      "\n",
      "Epoch: 31 of 100, train  Loss: 1.2718 Acc: 0.5700\n",
      "Epoch: 31 of 100, val    Loss: 1.2315 Acc: 0.6400\n",
      "\n",
      "Epoch: 32 of 100, train  Loss: 1.2634 Acc: 0.5800\n",
      "Epoch: 32 of 100, val    Loss: 1.2496 Acc: 0.6000\n",
      "\n",
      "Epoch: 33 of 100, train  Loss: 1.2826 Acc: 0.5700\n",
      "Epoch: 33 of 100, val    Loss: 1.2206 Acc: 0.6100\n",
      "\n",
      "Epoch: 34 of 100, train  Loss: 1.2994 Acc: 0.5400\n",
      "Epoch: 34 of 100, val    Loss: 1.1988 Acc: 0.6300\n",
      "\n",
      "Epoch: 35 of 100, train  Loss: 1.2833 Acc: 0.5500\n",
      "Epoch: 35 of 100, val    Loss: 1.2020 Acc: 0.6100\n",
      "\n",
      "Epoch: 36 of 100, train  Loss: 1.2602 Acc: 0.6000\n",
      "Epoch: 36 of 100, val    Loss: 1.2437 Acc: 0.6600\n",
      "\n",
      "Epoch: 37 of 100, train  Loss: 1.2806 Acc: 0.5700\n",
      "Epoch: 37 of 100, val    Loss: 1.2296 Acc: 0.6600\n",
      "\n",
      "Epoch: 38 of 100, train  Loss: 1.2183 Acc: 0.5700\n",
      "Epoch: 38 of 100, val    Loss: 1.1865 Acc: 0.6300\n",
      "\n",
      "Epoch: 39 of 100, train  Loss: 1.2763 Acc: 0.5200\n",
      "Epoch: 39 of 100, val    Loss: 1.1948 Acc: 0.6800\n",
      "\n",
      "Epoch: 40 of 100, train  Loss: 1.2677 Acc: 0.5700\n",
      "Epoch: 40 of 100, val    Loss: 1.2041 Acc: 0.6400\n",
      "\n",
      "Epoch: 41 of 100, train  Loss: 1.3094 Acc: 0.5200\n",
      "Epoch: 41 of 100, val    Loss: 1.2105 Acc: 0.6300\n",
      "\n",
      "Epoch: 42 of 100, train  Loss: 1.2740 Acc: 0.5700\n",
      "Epoch: 42 of 100, val    Loss: 1.1947 Acc: 0.6400\n",
      "\n",
      "Epoch: 43 of 100, train  Loss: 1.2702 Acc: 0.5400\n",
      "Epoch: 43 of 100, val    Loss: 1.2166 Acc: 0.5700\n",
      "\n",
      "Epoch: 44 of 100, train  Loss: 1.2608 Acc: 0.5700\n",
      "Epoch: 44 of 100, val    Loss: 1.2558 Acc: 0.6100\n",
      "\n",
      "Epoch: 45 of 100, train  Loss: 1.2793 Acc: 0.5900\n",
      "Epoch: 45 of 100, val    Loss: 1.1686 Acc: 0.6400\n",
      "\n",
      "Epoch: 46 of 100, train  Loss: 1.2600 Acc: 0.5700\n",
      "Epoch: 46 of 100, val    Loss: 1.2080 Acc: 0.6400\n",
      "\n",
      "Epoch: 47 of 100, train  Loss: 1.3305 Acc: 0.5200\n",
      "Epoch: 47 of 100, val    Loss: 1.1965 Acc: 0.6400\n",
      "\n",
      "Epoch: 48 of 100, train  Loss: 1.2794 Acc: 0.5800\n",
      "Epoch: 48 of 100, val    Loss: 1.1918 Acc: 0.6500\n",
      "\n",
      "Epoch: 49 of 100, train  Loss: 1.3015 Acc: 0.5200\n",
      "Epoch: 49 of 100, val    Loss: 1.2392 Acc: 0.6400\n",
      "\n",
      "Epoch: 50 of 100, train  Loss: 1.3120 Acc: 0.5300\n",
      "Epoch: 50 of 100, val    Loss: 1.2445 Acc: 0.6700\n",
      "\n",
      "Epoch: 51 of 100, train  Loss: 1.2804 Acc: 0.5000\n",
      "Epoch: 51 of 100, val    Loss: 1.1702 Acc: 0.6400\n",
      "\n",
      "Epoch: 52 of 100, train  Loss: 1.2472 Acc: 0.6000\n",
      "Epoch: 52 of 100, val    Loss: 1.2594 Acc: 0.6400\n",
      "\n",
      "Epoch: 53 of 100, train  Loss: 1.2758 Acc: 0.5800\n",
      "Epoch: 53 of 100, val    Loss: 1.2323 Acc: 0.6200\n",
      "\n",
      "Epoch: 54 of 100, train  Loss: 1.2492 Acc: 0.5800\n",
      "Epoch: 54 of 100, val    Loss: 1.2044 Acc: 0.6200\n",
      "\n",
      "Epoch: 55 of 100, train  Loss: 1.2964 Acc: 0.5400\n",
      "Epoch: 55 of 100, val    Loss: 1.2753 Acc: 0.5900\n",
      "\n",
      "Epoch: 56 of 100, train  Loss: 1.2438 Acc: 0.5700\n",
      "Epoch: 56 of 100, val    Loss: 1.1944 Acc: 0.6300\n",
      "\n",
      "Epoch: 57 of 100, train  Loss: 1.2952 Acc: 0.5100\n",
      "Epoch: 57 of 100, val    Loss: 1.1854 Acc: 0.6500\n",
      "\n",
      "Epoch: 58 of 100, train  Loss: 1.2692 Acc: 0.5800\n",
      "Epoch: 58 of 100, val    Loss: 1.2153 Acc: 0.6300\n",
      "\n",
      "Epoch: 59 of 100, train  Loss: 1.2591 Acc: 0.5300\n",
      "Epoch: 59 of 100, val    Loss: 1.2373 Acc: 0.6100\n",
      "\n",
      "Epoch: 60 of 100, train  Loss: 1.2547 Acc: 0.5800\n",
      "Epoch: 60 of 100, val    Loss: 1.2053 Acc: 0.6300\n",
      "\n",
      "Epoch: 61 of 100, train  Loss: 1.2617 Acc: 0.5700\n",
      "Epoch: 61 of 100, val    Loss: 1.2844 Acc: 0.6100\n",
      "\n",
      "Epoch: 62 of 100, train  Loss: 1.2597 Acc: 0.6100\n",
      "Epoch: 62 of 100, val    Loss: 1.2565 Acc: 0.6100\n",
      "\n",
      "Epoch: 63 of 100, train  Loss: 1.2182 Acc: 0.6000\n",
      "Epoch: 63 of 100, val    Loss: 1.2133 Acc: 0.6300\n",
      "\n",
      "Epoch: 64 of 100, train  Loss: 1.2921 Acc: 0.5400\n",
      "Epoch: 64 of 100, val    Loss: 1.2501 Acc: 0.6100\n",
      "\n",
      "Epoch: 65 of 100, train  Loss: 1.2834 Acc: 0.5700\n",
      "Epoch: 65 of 100, val    Loss: 1.2161 Acc: 0.5900\n",
      "\n",
      "Epoch: 66 of 100, train  Loss: 1.2949 Acc: 0.5500\n",
      "Epoch: 66 of 100, val    Loss: 1.2147 Acc: 0.6400\n",
      "\n",
      "Epoch: 67 of 100, train  Loss: 1.2569 Acc: 0.6100\n",
      "Epoch: 67 of 100, val    Loss: 1.2145 Acc: 0.6400\n",
      "\n",
      "Epoch: 68 of 100, train  Loss: 1.2361 Acc: 0.5900\n",
      "Epoch: 68 of 100, val    Loss: 1.1961 Acc: 0.5900\n",
      "\n",
      "Epoch: 69 of 100, train  Loss: 1.2454 Acc: 0.5700\n",
      "Epoch: 69 of 100, val    Loss: 1.2413 Acc: 0.5900\n",
      "\n",
      "Epoch: 70 of 100, train  Loss: 1.2510 Acc: 0.6200\n",
      "Epoch: 70 of 100, val    Loss: 1.1986 Acc: 0.6300\n",
      "\n",
      "Epoch: 71 of 100, train  Loss: 1.3083 Acc: 0.4800\n",
      "Epoch: 71 of 100, val    Loss: 1.2758 Acc: 0.6100\n",
      "\n",
      "Epoch: 72 of 100, train  Loss: 1.2543 Acc: 0.5300\n",
      "Epoch: 72 of 100, val    Loss: 1.2409 Acc: 0.6100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 of 100, train  Loss: 1.2971 Acc: 0.5500\n",
      "Epoch: 73 of 100, val    Loss: 1.2527 Acc: 0.6100\n",
      "\n",
      "Epoch: 74 of 100, train  Loss: 1.2343 Acc: 0.5700\n",
      "Epoch: 74 of 100, val    Loss: 1.1984 Acc: 0.6300\n",
      "\n",
      "Epoch: 75 of 100, train  Loss: 1.3043 Acc: 0.5200\n",
      "Epoch: 75 of 100, val    Loss: 1.1925 Acc: 0.6500\n",
      "\n",
      "Epoch: 76 of 100, train  Loss: 1.3039 Acc: 0.5400\n",
      "Epoch: 76 of 100, val    Loss: 1.2254 Acc: 0.6200\n",
      "\n",
      "Epoch: 77 of 100, train  Loss: 1.2899 Acc: 0.5200\n",
      "Epoch: 77 of 100, val    Loss: 1.2467 Acc: 0.5900\n",
      "\n",
      "Epoch: 78 of 100, train  Loss: 1.2800 Acc: 0.5300\n",
      "Epoch: 78 of 100, val    Loss: 1.1951 Acc: 0.5600\n",
      "\n",
      "Epoch: 79 of 100, train  Loss: 1.2411 Acc: 0.6000\n",
      "Epoch: 79 of 100, val    Loss: 1.2300 Acc: 0.6800\n",
      "\n",
      "Epoch: 80 of 100, train  Loss: 1.2708 Acc: 0.5500\n",
      "Epoch: 80 of 100, val    Loss: 1.1990 Acc: 0.7200\n",
      "\n",
      "Epoch: 81 of 100, train  Loss: 1.2168 Acc: 0.6600\n",
      "Epoch: 81 of 100, val    Loss: 1.1743 Acc: 0.6300\n",
      "\n",
      "Epoch: 82 of 100, train  Loss: 1.2625 Acc: 0.5700\n",
      "Epoch: 82 of 100, val    Loss: 1.2159 Acc: 0.6400\n",
      "\n",
      "Epoch: 83 of 100, train  Loss: 1.2306 Acc: 0.6200\n",
      "Epoch: 83 of 100, val    Loss: 1.2096 Acc: 0.5600\n",
      "\n",
      "Epoch: 84 of 100, train  Loss: 1.2719 Acc: 0.5700\n",
      "Epoch: 84 of 100, val    Loss: 1.2462 Acc: 0.6000\n",
      "\n",
      "Epoch: 85 of 100, train  Loss: 1.2394 Acc: 0.5900\n",
      "Epoch: 85 of 100, val    Loss: 1.2362 Acc: 0.6100\n",
      "\n",
      "Epoch: 86 of 100, train  Loss: 1.2904 Acc: 0.5100\n",
      "Epoch: 86 of 100, val    Loss: 1.2004 Acc: 0.6400\n",
      "\n",
      "Epoch: 87 of 100, train  Loss: 1.2728 Acc: 0.5200\n",
      "Epoch: 87 of 100, val    Loss: 1.2231 Acc: 0.6300\n",
      "\n",
      "Epoch: 88 of 100, train  Loss: 1.3156 Acc: 0.5500\n",
      "Epoch: 88 of 100, val    Loss: 1.2463 Acc: 0.5900\n",
      "\n",
      "Epoch: 89 of 100, train  Loss: 1.2672 Acc: 0.6000\n",
      "Epoch: 89 of 100, val    Loss: 1.1960 Acc: 0.6200\n",
      "\n",
      "Epoch: 90 of 100, train  Loss: 1.2836 Acc: 0.5400\n",
      "Epoch: 90 of 100, val    Loss: 1.2128 Acc: 0.6300\n",
      "\n",
      "Epoch: 91 of 100, train  Loss: 1.3052 Acc: 0.4700\n",
      "Epoch: 91 of 100, val    Loss: 1.2135 Acc: 0.6600\n",
      "\n",
      "Epoch: 92 of 100, train  Loss: 1.2644 Acc: 0.5800\n",
      "Epoch: 92 of 100, val    Loss: 1.1742 Acc: 0.6600\n",
      "\n",
      "Epoch: 93 of 100, train  Loss: 1.3017 Acc: 0.5600\n",
      "Epoch: 93 of 100, val    Loss: 1.2419 Acc: 0.6400\n",
      "\n",
      "Epoch: 94 of 100, train  Loss: 1.2728 Acc: 0.5500\n",
      "Epoch: 94 of 100, val    Loss: 1.2172 Acc: 0.6700\n",
      "\n",
      "Epoch: 95 of 100, train  Loss: 1.2427 Acc: 0.5900\n",
      "Epoch: 95 of 100, val    Loss: 1.2072 Acc: 0.6100\n",
      "\n",
      "Epoch: 96 of 100, train  Loss: 1.2644 Acc: 0.6100\n",
      "Epoch: 96 of 100, val    Loss: 1.2137 Acc: 0.6800\n",
      "\n",
      "Epoch: 97 of 100, train  Loss: 1.2848 Acc: 0.5800\n",
      "Epoch: 97 of 100, val    Loss: 1.2039 Acc: 0.6300\n",
      "\n",
      "Epoch: 98 of 100, train  Loss: 1.2782 Acc: 0.5100\n",
      "Epoch: 98 of 100, val    Loss: 1.2759 Acc: 0.6400\n",
      "\n",
      "Epoch: 99 of 100, train  Loss: 1.2896 Acc: 0.5700\n",
      "Epoch: 99 of 100, val    Loss: 1.2067 Acc: 0.6500\n",
      "\n",
      "Epoch: 100 of 100, train  Loss: 1.2520 Acc: 0.5500\n",
      "Epoch: 100 of 100, val    Loss: 1.2145 Acc: 0.6600\n",
      "\n",
      "Training complete in 7m 50s\n",
      "Best val Acc: 0.6800 Best val loss: 1.1578\n",
      "Wall time: 7min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#data = dataloaders\n",
    "data = dataloaders_reduced\n",
    "\n",
    "model_conv, best_val_loss, best_val_acc = train_model(data,\n",
    "                                                      model_conv,\n",
    "                                                      criterion,\n",
    "                                                      optimizer_conv,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      num_epochs = 100,\n",
    "                                                      checkpoint = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3WHcYME2huF"
   },
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model_conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_conv.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQuhWHVqVJE_"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ase07EAFAHwO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is A49E-46A6\n",
      "\n",
      " Directory of C:\\Labs\\DomainAdaptation\n",
      "\n",
      "04/26/2020  11:33 AM    <DIR>          .\n",
      "04/26/2020  11:33 AM    <DIR>          ..\n",
      "04/24/2020  01:14 AM    <DIR>          .idea\n",
      "04/23/2020  05:33 PM    <DIR>          .ipynb_checkpoints\n",
      "04/26/2020  11:33 AM            42,807 NetWithDataLoader.ipynb\n",
      "04/19/2020  12:11 PM                34 README.md\n",
      "04/24/2020  12:35 AM             4,577 test_iterator.py\n",
      "               3 File(s)         47,418 bytes\n",
      "               4 Dir(s)  200,540,536,832 bytes free\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Angry\n",
      "Counter({'Angry': 11})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "x='train'\n",
    "d = datasets.ImageFolder(os.path.join(DATA_DIR, x))\n",
    "cnt = Counter([])\n",
    "for i,(image,category) in enumerate(d):\n",
    "    cnt.update({(image_datasets['train'].classes)[category]:1})\n",
    "print(cnt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angry'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train'].classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NetWithDataLoader.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "output_auto_scroll": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08e0b965fc614feb960cf954223db3e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e81eb8a8cdb40738fb85ce51cfed6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08e0b965fc614feb960cf954223db3e3",
      "placeholder": "​",
      "style": "IPY_MODEL_6d2c8e3068854041be63a92194b35a64",
      "value": " 44.7M/44.7M [11:04&lt;00:00, 70.5kB/s]"
     }
    },
    "3876f5ef3ff947b7b26c72b6512166f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa2dc4675ff940b48a90425981eb55a6",
       "IPY_MODEL_2e81eb8a8cdb40738fb85ce51cfed6c7"
      ],
      "layout": "IPY_MODEL_79f9ecfbdd9a4871b95ae9668720c847"
     }
    },
    "6d2c8e3068854041be63a92194b35a64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79f9ecfbdd9a4871b95ae9668720c847": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "966b7f8371654dd2a54f484c4a9c0129": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a315579ab89f43e2b7d184c7c4d64baa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa2dc4675ff940b48a90425981eb55a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a315579ab89f43e2b7d184c7c4d64baa",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_966b7f8371654dd2a54f484c4a9c0129",
      "value": 46827520
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
