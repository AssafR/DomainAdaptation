{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "npGiuvftxf1z",
    "outputId": "48a8cb76-f300-4db3-f216-eba77d4cce90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.18</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import itertools\n",
    "import pixiedust\n",
    "import random\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "plt.ion()   # interactive mode\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "MHoa2aySwBF_",
    "outputId": "31d7f712-1312-4029-8773-9d4b77d44312"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    LABS_DIR = Path ('/content/gdrive/My Drive/Labs')\n",
    "except:\n",
    "    LABS_DIR = Path ('C:/Labs/')\n",
    "\n",
    "DATA_DIR = LABS_DIR/'Data'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRBBa4e-xRBD"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# for validatin we use normalization and resize (for train we also change the angle and size of the images)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "hmElu6Slwjgf",
    "outputId": "6b438b4a-4913-46c6-9bc5-6884fcfa2d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
      "Train image size: 100\n",
      "Validation image size: 100\n"
     ]
    }
   ],
   "source": [
    "''' The function takes the data loader and a parameter  '''\n",
    "def create_train_val_slice(data,train_limit=None,val_same_as_train=False, val_limit=None):\n",
    "    train1, train2 = itertools.tee(islice(data['train'],train_limit))\n",
    "    result_slice = {}\n",
    "    result_slice['train'] = train1\n",
    "    if val_same_as_train:\n",
    "        result_slice['val'] = train2\n",
    "    else:\n",
    "        result_slice['val'] = islice(data['val'],val_limit)\n",
    "    return result_slice\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "#                                               shuffle=True, num_workers=1)\n",
    "#               for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "sample_size = 100\n",
    "sample_n = {x: random.sample(list(range(dataset_sizes[x])), sample_size)\n",
    "            for x in ['train', 'val']}\n",
    "\n",
    "image_datasets_reduced = {x: torch.utils.data.Subset(image_datasets['train'], sample_n['train'])\n",
    "                          for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets_reduced[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders_reduced = {x: torch.utils.data.DataLoader(image_datasets_reduced[x], batch_size=8,\n",
    "                                              shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "#data = dataloaders\n",
    "data = dataloaders_reduced\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Classes: \", class_names) \n",
    "print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
    "print(f'Validation image size: {dataset_sizes[\"val\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "aOMEb5deCofk",
    "outputId": "40524968-328f-420d-cfc1-1e3fe2632a01"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "# # Make a grid from batch\n",
    "# sample_train_images = torchvision.utils.make_grid(inputs)\n",
    "# #imshow(sample_train_images, title=classes)\n",
    "# print(f\"classes={classes}\")\n",
    "# imshow(sample_train_images, title=[class_names[i] for i in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     25,
     27,
     79
    ],
    "colab": {},
    "colab_type": "code",
    "id": "sDNvKgitDNG7",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data, model, criterion, optimizer, scheduler, num_epochs=2, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "    if checkpoint is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_loss = math.inf\n",
    "        best_acc = 0.\n",
    "    else:\n",
    "        print('Val loss: {}, Val accuracy: {}'\\\n",
    "              .format(checkpoint[\"best_val_loss\"], checkpoint[\"best_val_accuracy\"]))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        best_loss = checkpoint['best_val_loss']\n",
    "        best_acc = checkpoint['best_val_accuracy']\n",
    "           \n",
    "    print(\"Starting epochs\")\n",
    "    outer = tqdm(total=num_epochs, desc='Epoch', position=0)\n",
    "    inner = tqdm(total=500, position=1)\n",
    "    for epoch in range(num_epochs):\n",
    "        outer.update(1)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            total = 500\n",
    "\n",
    "            # Handle tqdm inner loop counter\n",
    "            inner.total = dataset_sizes[phase]\n",
    "            inner.reset()\n",
    "            inner_total = 0 \n",
    "            \n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data[phase]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if i>0:\n",
    "                    report_str = '[%d, %d] loss: %.3f' % (epoch + 1, i, running_loss / i * inputs.size(0))\n",
    "                else:\n",
    "                    report_str = \"first iteration\"\n",
    "\n",
    "                # Update inner tqdm, we are about to override the previous maximum, update maximum\n",
    "                if inner_total >= total:\n",
    "                    total = total *2\n",
    "                    inner.total = total\n",
    "                inner_total = inner_total + 1\n",
    "                inner.update(1) # Advance the tqdm counter\n",
    "                inner.desc = f'Phase: {phase} ' + report_str\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                # print(\"running_corrects =\", running_corrects)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                \n",
    "            #inner.write(\"running_corrects=\", running_corrects, \" epoch: \", epoch)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            loss_str = f'Epoch: {epoch+1} of {num_epochs}, {phase:6} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}'\n",
    "            inner.write(loss_str)\n",
    "            \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print('New best model found!')\n",
    "                print('New record loss:{}, previous record loss: {}'.format(epoch_loss, best_loss))\n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                #save the weights\n",
    "                torch.save(best_model_wts, CHECK_POINT_PATH)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:.4f} Best val loss: {:.4f}'.format(best_acc, best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "3876f5ef3ff947b7b26c72b6512166f8",
      "79f9ecfbdd9a4871b95ae9668720c847",
      "fa2dc4675ff940b48a90425981eb55a6",
      "2e81eb8a8cdb40738fb85ce51cfed6c7",
      "966b7f8371654dd2a54f484c4a9c0129",
      "a315579ab89f43e2b7d184c7c4d64baa",
      "6d2c8e3068854041be63a92194b35a64",
      "08e0b965fc614feb960cf954223db3e3"
     ]
    },
    "colab_type": "code",
    "id": "AUF-cgM1y_7L",
    "outputId": "107315c6-103a-4192-a611-d3e76809d530"
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "#model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "#model_conv = torchvision.models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_conv.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35uVilmuNgg-"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UgElP8XDrYm"
   },
   "outputs": [],
   "source": [
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "\n",
    "ct = 0\n",
    "for child in model_conv.children():\n",
    "  ct += 1\n",
    "  # freezes layers 1-6 in the total 10 layers of Resnet50\n",
    "  if ct < 7:\n",
    "    for param in child.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "'''two options to write the loss. They are both equal'''\n",
    "# option 1 #\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# option 2 #\n",
    "# p = nn.functional.softmax(model_conv, dim=1)\n",
    "# # to calculate loss using probabilities you can do below \n",
    "# criterion = nn.functional.nll_loss(torch.log(p), y)\n",
    "\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IaCCoou62TKD",
    "outputId": "058f59dd-7db0-4456-f29d-42ae2d4567c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint not found\n"
     ]
    }
   ],
   "source": [
    "CHECK_POINT_PATH = LABS_DIR/'ModelParams'/'checkpoint.tar'\n",
    "\n",
    "# !del $CHECK_POI NT_PATH\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "    print(\"checkpoint loaded\")\n",
    "except:\n",
    "    checkpoint = None\n",
    "    print(\"checkpoint not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_tKaq9vU0cjq",
    "outputId": "1b3c5122-2b8f-4c1c-cedc-811cbfe8fcfe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeb6b48c0c34416b01ac055bf67827f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', style=ProgressStyle(description_width='initial'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c758e8164604fee83118315e0eb8949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 of 100, train  Loss: 1.9204 Acc: 0.2600\n",
      "Epoch: 1 of 100, val    Loss: 1.7812 Acc: 0.2900\n",
      "New best model found!\n",
      "New record loss:1.7812447881698608, previous record loss: inf\n",
      "\n",
      "Epoch: 2 of 100, train  Loss: 1.8949 Acc: 0.1800\n",
      "Epoch: 2 of 100, val    Loss: 1.8654 Acc: 0.2100\n",
      "\n",
      "Epoch: 3 of 100, train  Loss: 1.9410 Acc: 0.1700\n",
      "Epoch: 3 of 100, val    Loss: 1.7248 Acc: 0.3500\n",
      "New best model found!\n",
      "New record loss:1.7247825765609741, previous record loss: 1.7812447881698608\n",
      "\n",
      "Epoch: 4 of 100, train  Loss: 1.5944 Acc: 0.3100\n",
      "Epoch: 4 of 100, val    Loss: 1.5497 Acc: 0.3400\n",
      "New best model found!\n",
      "New record loss:1.5497144746780396, previous record loss: 1.7247825765609741\n",
      "\n",
      "Epoch: 5 of 100, train  Loss: 1.7597 Acc: 0.3700\n",
      "Epoch: 5 of 100, val    Loss: 1.4610 Acc: 0.4900\n",
      "New best model found!\n",
      "New record loss:1.4609520435333252, previous record loss: 1.5497144746780396\n",
      "\n",
      "Epoch: 6 of 100, train  Loss: 1.5716 Acc: 0.4200\n",
      "Epoch: 6 of 100, val    Loss: 1.4020 Acc: 0.4600\n",
      "New best model found!\n",
      "New record loss:1.4020420551300048, previous record loss: 1.4609520435333252\n",
      "\n",
      "Epoch: 7 of 100, train  Loss: 1.4349 Acc: 0.5600\n",
      "Epoch: 7 of 100, val    Loss: 1.2991 Acc: 0.5300\n",
      "New best model found!\n",
      "New record loss:1.2991197538375854, previous record loss: 1.4020420551300048\n",
      "\n",
      "Epoch: 8 of 100, train  Loss: 1.3793 Acc: 0.4700\n",
      "Epoch: 8 of 100, val    Loss: 1.2903 Acc: 0.5400\n",
      "New best model found!\n",
      "New record loss:1.2903107357025148, previous record loss: 1.2991197538375854\n",
      "\n",
      "Epoch: 9 of 100, train  Loss: 1.3241 Acc: 0.5400\n",
      "Epoch: 9 of 100, val    Loss: 1.3109 Acc: 0.5700\n",
      "\n",
      "Epoch: 10 of 100, train  Loss: 1.3199 Acc: 0.5000\n",
      "Epoch: 10 of 100, val    Loss: 1.2845 Acc: 0.5800\n",
      "New best model found!\n",
      "New record loss:1.2844756746292114, previous record loss: 1.2903107357025148\n",
      "\n",
      "Epoch: 11 of 100, train  Loss: 1.3684 Acc: 0.4800\n",
      "Epoch: 11 of 100, val    Loss: 1.2432 Acc: 0.5400\n",
      "New best model found!\n",
      "New record loss:1.2432013034820557, previous record loss: 1.2844756746292114\n",
      "\n",
      "Epoch: 12 of 100, train  Loss: 1.3566 Acc: 0.4400\n",
      "Epoch: 12 of 100, val    Loss: 1.2539 Acc: 0.5600\n",
      "\n",
      "Epoch: 13 of 100, train  Loss: 1.3639 Acc: 0.5300\n",
      "Epoch: 13 of 100, val    Loss: 1.2438 Acc: 0.5900\n",
      "\n",
      "Epoch: 14 of 100, train  Loss: 1.3411 Acc: 0.4800\n",
      "Epoch: 14 of 100, val    Loss: 1.2531 Acc: 0.5200\n",
      "\n",
      "Epoch: 15 of 100, train  Loss: 1.3637 Acc: 0.5100\n",
      "Epoch: 15 of 100, val    Loss: 1.2674 Acc: 0.5800\n",
      "\n",
      "Epoch: 16 of 100, train  Loss: 1.3361 Acc: 0.5300\n",
      "Epoch: 16 of 100, val    Loss: 1.2768 Acc: 0.6000\n",
      "\n",
      "Epoch: 17 of 100, train  Loss: 1.3505 Acc: 0.5400\n",
      "Epoch: 17 of 100, val    Loss: 1.2585 Acc: 0.5500\n",
      "\n",
      "Epoch: 18 of 100, train  Loss: 1.3424 Acc: 0.5000\n",
      "Epoch: 18 of 100, val    Loss: 1.2552 Acc: 0.5500\n",
      "\n",
      "Epoch: 19 of 100, train  Loss: 1.3190 Acc: 0.5800\n",
      "Epoch: 19 of 100, val    Loss: 1.2822 Acc: 0.5700\n",
      "\n",
      "Epoch: 20 of 100, train  Loss: 1.2938 Acc: 0.5700\n",
      "Epoch: 20 of 100, val    Loss: 1.2636 Acc: 0.5600\n",
      "\n",
      "Epoch: 21 of 100, train  Loss: 1.3238 Acc: 0.5400\n",
      "Epoch: 21 of 100, val    Loss: 1.2125 Acc: 0.6100\n",
      "New best model found!\n",
      "New record loss:1.2124840927124023, previous record loss: 1.2432013034820557\n",
      "\n",
      "Epoch: 22 of 100, train  Loss: 1.3191 Acc: 0.5000\n",
      "Epoch: 22 of 100, val    Loss: 1.2724 Acc: 0.5500\n",
      "\n",
      "Epoch: 23 of 100, train  Loss: 1.3580 Acc: 0.5000\n",
      "Epoch: 23 of 100, val    Loss: 1.2834 Acc: 0.6200\n",
      "\n",
      "Epoch: 24 of 100, train  Loss: 1.3389 Acc: 0.5300\n",
      "Epoch: 24 of 100, val    Loss: 1.2680 Acc: 0.5600\n",
      "\n",
      "Epoch: 25 of 100, train  Loss: 1.3494 Acc: 0.5200\n",
      "Epoch: 25 of 100, val    Loss: 1.3115 Acc: 0.4400\n",
      "\n",
      "Epoch: 26 of 100, train  Loss: 1.3691 Acc: 0.5100\n",
      "Epoch: 26 of 100, val    Loss: 1.2387 Acc: 0.6100\n",
      "\n",
      "Epoch: 27 of 100, train  Loss: 1.2947 Acc: 0.5500\n",
      "Epoch: 27 of 100, val    Loss: 1.2525 Acc: 0.5600\n",
      "\n",
      "Epoch: 28 of 100, train  Loss: 1.3415 Acc: 0.5000\n",
      "Epoch: 28 of 100, val    Loss: 1.2650 Acc: 0.5600\n",
      "\n",
      "Epoch: 29 of 100, train  Loss: 1.3079 Acc: 0.5100\n",
      "Epoch: 29 of 100, val    Loss: 1.1958 Acc: 0.6100\n",
      "New best model found!\n",
      "New record loss:1.195837893486023, previous record loss: 1.2124840927124023\n",
      "\n",
      "Epoch: 30 of 100, train  Loss: 1.3427 Acc: 0.5100\n",
      "Epoch: 30 of 100, val    Loss: 1.2272 Acc: 0.6100\n",
      "\n",
      "Epoch: 31 of 100, train  Loss: 1.3333 Acc: 0.5100\n",
      "Epoch: 31 of 100, val    Loss: 1.2303 Acc: 0.6100\n",
      "\n",
      "Epoch: 32 of 100, train  Loss: 1.3661 Acc: 0.4800\n",
      "Epoch: 32 of 100, val    Loss: 1.2297 Acc: 0.5600\n",
      "\n",
      "Epoch: 33 of 100, train  Loss: 1.3273 Acc: 0.5500\n",
      "Epoch: 33 of 100, val    Loss: 1.3008 Acc: 0.5200\n",
      "\n",
      "Epoch: 34 of 100, train  Loss: 1.2887 Acc: 0.5500\n",
      "Epoch: 34 of 100, val    Loss: 1.2031 Acc: 0.6000\n",
      "\n",
      "Epoch: 35 of 100, train  Loss: 1.3182 Acc: 0.5100\n",
      "Epoch: 35 of 100, val    Loss: 1.2326 Acc: 0.5900\n",
      "\n",
      "Epoch: 36 of 100, train  Loss: 1.3552 Acc: 0.5000\n",
      "Epoch: 36 of 100, val    Loss: 1.2206 Acc: 0.6300\n",
      "\n",
      "Epoch: 37 of 100, train  Loss: 1.3202 Acc: 0.5300\n",
      "Epoch: 37 of 100, val    Loss: 1.2721 Acc: 0.5300\n",
      "\n",
      "Epoch: 38 of 100, train  Loss: 1.3268 Acc: 0.5100\n",
      "Epoch: 38 of 100, val    Loss: 1.2302 Acc: 0.5700\n",
      "\n",
      "Epoch: 39 of 100, train  Loss: 1.3405 Acc: 0.5500\n",
      "Epoch: 39 of 100, val    Loss: 1.2335 Acc: 0.6200\n",
      "\n",
      "Epoch: 40 of 100, train  Loss: 1.3175 Acc: 0.5500\n",
      "Epoch: 40 of 100, val    Loss: 1.2490 Acc: 0.5700\n",
      "\n",
      "Epoch: 41 of 100, train  Loss: 1.3462 Acc: 0.4900\n",
      "Epoch: 41 of 100, val    Loss: 1.2573 Acc: 0.6100\n",
      "\n",
      "Epoch: 42 of 100, train  Loss: 1.2726 Acc: 0.6300\n",
      "Epoch: 42 of 100, val    Loss: 1.2352 Acc: 0.6100\n",
      "\n",
      "Epoch: 43 of 100, train  Loss: 1.3400 Acc: 0.4900\n",
      "Epoch: 43 of 100, val    Loss: 1.2883 Acc: 0.5400\n",
      "\n",
      "Epoch: 44 of 100, train  Loss: 1.3439 Acc: 0.5400\n",
      "Epoch: 44 of 100, val    Loss: 1.2351 Acc: 0.6500\n",
      "\n",
      "Epoch: 45 of 100, train  Loss: 1.3575 Acc: 0.4400\n",
      "Epoch: 45 of 100, val    Loss: 1.2421 Acc: 0.6000\n",
      "\n",
      "Epoch: 46 of 100, train  Loss: 1.3297 Acc: 0.4800\n",
      "Epoch: 46 of 100, val    Loss: 1.2485 Acc: 0.5600\n",
      "\n",
      "Epoch: 47 of 100, train  Loss: 1.3761 Acc: 0.4600\n",
      "Epoch: 47 of 100, val    Loss: 1.2878 Acc: 0.5900\n",
      "\n",
      "Epoch: 48 of 100, train  Loss: 1.2774 Acc: 0.5600\n",
      "Epoch: 48 of 100, val    Loss: 1.2792 Acc: 0.6100\n",
      "\n",
      "Epoch: 49 of 100, train  Loss: 1.2864 Acc: 0.5500\n",
      "Epoch: 49 of 100, val    Loss: 1.2475 Acc: 0.5900\n",
      "\n",
      "Epoch: 50 of 100, train  Loss: 1.3220 Acc: 0.5600\n",
      "Epoch: 50 of 100, val    Loss: 1.2665 Acc: 0.5700\n",
      "\n",
      "Epoch: 51 of 100, train  Loss: 1.3531 Acc: 0.5400\n",
      "Epoch: 51 of 100, val    Loss: 1.2480 Acc: 0.6300\n",
      "\n",
      "Epoch: 52 of 100, train  Loss: 1.3747 Acc: 0.4400\n",
      "Epoch: 52 of 100, val    Loss: 1.2679 Acc: 0.6200\n",
      "\n",
      "Epoch: 53 of 100, train  Loss: 1.3138 Acc: 0.5500\n",
      "Epoch: 53 of 100, val    Loss: 1.2462 Acc: 0.6100\n",
      "\n",
      "Epoch: 54 of 100, train  Loss: 1.3288 Acc: 0.5500\n",
      "Epoch: 54 of 100, val    Loss: 1.2413 Acc: 0.5700\n",
      "\n",
      "Epoch: 55 of 100, train  Loss: 1.2572 Acc: 0.5800\n",
      "Epoch: 55 of 100, val    Loss: 1.2206 Acc: 0.6500\n",
      "\n",
      "Epoch: 56 of 100, train  Loss: 1.3183 Acc: 0.5600\n",
      "Epoch: 56 of 100, val    Loss: 1.2274 Acc: 0.6100\n",
      "\n",
      "Epoch: 57 of 100, train  Loss: 1.3297 Acc: 0.5800\n",
      "Epoch: 57 of 100, val    Loss: 1.2388 Acc: 0.6000\n",
      "\n",
      "Epoch: 58 of 100, train  Loss: 1.3921 Acc: 0.4600\n",
      "Epoch: 58 of 100, val    Loss: 1.2699 Acc: 0.5800\n",
      "\n",
      "Epoch: 59 of 100, train  Loss: 1.3222 Acc: 0.4900\n",
      "Epoch: 59 of 100, val    Loss: 1.2674 Acc: 0.5900\n",
      "\n",
      "Epoch: 60 of 100, train  Loss: 1.3829 Acc: 0.5500\n",
      "Epoch: 60 of 100, val    Loss: 1.2872 Acc: 0.5200\n",
      "\n",
      "Epoch: 61 of 100, train  Loss: 1.4340 Acc: 0.4600\n",
      "Epoch: 61 of 100, val    Loss: 1.2301 Acc: 0.5700\n",
      "\n",
      "Epoch: 62 of 100, train  Loss: 1.3539 Acc: 0.4900\n",
      "Epoch: 62 of 100, val    Loss: 1.2495 Acc: 0.5800\n",
      "\n",
      "Epoch: 63 of 100, train  Loss: 1.3770 Acc: 0.4400\n",
      "Epoch: 63 of 100, val    Loss: 1.2132 Acc: 0.6600\n",
      "\n",
      "Epoch: 64 of 100, train  Loss: 1.3388 Acc: 0.5000\n",
      "Epoch: 64 of 100, val    Loss: 1.2423 Acc: 0.6400\n",
      "\n",
      "Epoch: 65 of 100, train  Loss: 1.3001 Acc: 0.5300\n",
      "Epoch: 65 of 100, val    Loss: 1.2575 Acc: 0.5900\n",
      "\n",
      "Epoch: 66 of 100, train  Loss: 1.2788 Acc: 0.6000\n",
      "Epoch: 66 of 100, val    Loss: 1.2783 Acc: 0.5500\n",
      "\n",
      "Epoch: 67 of 100, train  Loss: 1.3170 Acc: 0.5200\n",
      "Epoch: 67 of 100, val    Loss: 1.2576 Acc: 0.5700\n",
      "\n",
      "Epoch: 68 of 100, train  Loss: 1.3224 Acc: 0.5500\n",
      "Epoch: 68 of 100, val    Loss: 1.2289 Acc: 0.5700\n",
      "\n",
      "Epoch: 69 of 100, train  Loss: 1.3182 Acc: 0.5300\n",
      "Epoch: 69 of 100, val    Loss: 1.2525 Acc: 0.5800\n",
      "\n",
      "Epoch: 70 of 100, train  Loss: 1.3324 Acc: 0.5200\n",
      "Epoch: 70 of 100, val    Loss: 1.2787 Acc: 0.5500\n",
      "\n",
      "Epoch: 71 of 100, train  Loss: 1.3192 Acc: 0.5700\n",
      "Epoch: 71 of 100, val    Loss: 1.2733 Acc: 0.5500\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 of 100, train  Loss: 1.3215 Acc: 0.4900\n",
      "Epoch: 72 of 100, val    Loss: 1.2773 Acc: 0.5500\n",
      "\n",
      "Epoch: 73 of 100, train  Loss: 1.2948 Acc: 0.5600\n",
      "Epoch: 73 of 100, val    Loss: 1.2738 Acc: 0.5500\n",
      "\n",
      "Epoch: 74 of 100, train  Loss: 1.2702 Acc: 0.6200\n",
      "Epoch: 74 of 100, val    Loss: 1.2545 Acc: 0.5400\n",
      "\n",
      "Epoch: 75 of 100, train  Loss: 1.3215 Acc: 0.5700\n",
      "Epoch: 75 of 100, val    Loss: 1.2596 Acc: 0.6200\n",
      "\n",
      "Epoch: 76 of 100, train  Loss: 1.2951 Acc: 0.5900\n",
      "Epoch: 76 of 100, val    Loss: 1.2333 Acc: 0.5900\n",
      "\n",
      "Epoch: 77 of 100, train  Loss: 1.3300 Acc: 0.5000\n",
      "Epoch: 77 of 100, val    Loss: 1.2632 Acc: 0.5600\n",
      "\n",
      "Epoch: 78 of 100, train  Loss: 1.3222 Acc: 0.5000\n",
      "Epoch: 78 of 100, val    Loss: 1.2477 Acc: 0.6000\n",
      "\n",
      "Epoch: 79 of 100, train  Loss: 1.3583 Acc: 0.4900\n",
      "Epoch: 79 of 100, val    Loss: 1.2627 Acc: 0.5400\n",
      "\n",
      "Epoch: 80 of 100, train  Loss: 1.3728 Acc: 0.5300\n",
      "Epoch: 80 of 100, val    Loss: 1.2467 Acc: 0.5700\n",
      "\n",
      "Epoch: 81 of 100, train  Loss: 1.2752 Acc: 0.5200\n",
      "Epoch: 81 of 100, val    Loss: 1.2380 Acc: 0.5800\n",
      "\n",
      "Epoch: 82 of 100, train  Loss: 1.3099 Acc: 0.5600\n",
      "Epoch: 82 of 100, val    Loss: 1.2164 Acc: 0.5900\n",
      "\n",
      "Epoch: 83 of 100, train  Loss: 1.3401 Acc: 0.5300\n",
      "Epoch: 83 of 100, val    Loss: 1.2314 Acc: 0.6300\n",
      "\n",
      "Epoch: 84 of 100, train  Loss: 1.3554 Acc: 0.5100\n",
      "Epoch: 84 of 100, val    Loss: 1.2575 Acc: 0.6400\n",
      "\n",
      "Epoch: 85 of 100, train  Loss: 1.3388 Acc: 0.5200\n",
      "Epoch: 85 of 100, val    Loss: 1.2527 Acc: 0.5500\n",
      "\n",
      "Epoch: 86 of 100, train  Loss: 1.2744 Acc: 0.5100\n",
      "Epoch: 86 of 100, val    Loss: 1.2281 Acc: 0.5900\n",
      "\n",
      "Epoch: 87 of 100, train  Loss: 1.3089 Acc: 0.5800\n",
      "Epoch: 87 of 100, val    Loss: 1.2889 Acc: 0.5700\n",
      "\n",
      "Epoch: 88 of 100, train  Loss: 1.2945 Acc: 0.5600\n",
      "Epoch: 88 of 100, val    Loss: 1.2919 Acc: 0.5700\n",
      "\n",
      "Epoch: 89 of 100, train  Loss: 1.3079 Acc: 0.5600\n",
      "Epoch: 89 of 100, val    Loss: 1.2843 Acc: 0.5200\n",
      "\n",
      "Epoch: 90 of 100, train  Loss: 1.3564 Acc: 0.5400\n",
      "Epoch: 90 of 100, val    Loss: 1.2589 Acc: 0.6100\n",
      "\n",
      "Epoch: 91 of 100, train  Loss: 1.3489 Acc: 0.5000\n",
      "Epoch: 91 of 100, val    Loss: 1.2847 Acc: 0.5500\n",
      "\n",
      "Epoch: 92 of 100, train  Loss: 1.3565 Acc: 0.5200\n",
      "Epoch: 92 of 100, val    Loss: 1.2750 Acc: 0.5600\n",
      "\n",
      "Epoch: 93 of 100, train  Loss: 1.3014 Acc: 0.6000\n",
      "Epoch: 93 of 100, val    Loss: 1.2557 Acc: 0.5800\n",
      "\n",
      "Epoch: 94 of 100, train  Loss: 1.2737 Acc: 0.5500\n",
      "Epoch: 94 of 100, val    Loss: 1.2403 Acc: 0.6100\n",
      "\n",
      "Epoch: 95 of 100, train  Loss: 1.3401 Acc: 0.5400\n",
      "Epoch: 95 of 100, val    Loss: 1.2448 Acc: 0.5800\n",
      "\n",
      "Epoch: 96 of 100, train  Loss: 1.3353 Acc: 0.5100\n",
      "Epoch: 96 of 100, val    Loss: 1.2631 Acc: 0.6100\n",
      "\n",
      "Epoch: 97 of 100, train  Loss: 1.3071 Acc: 0.4900\n",
      "Epoch: 97 of 100, val    Loss: 1.2340 Acc: 0.5900\n",
      "\n",
      "Epoch: 98 of 100, train  Loss: 1.3376 Acc: 0.4900\n",
      "Epoch: 98 of 100, val    Loss: 1.2654 Acc: 0.5600\n",
      "\n",
      "Epoch: 99 of 100, train  Loss: 1.3070 Acc: 0.6100\n",
      "Epoch: 99 of 100, val    Loss: 1.2410 Acc: 0.5700\n",
      "\n",
      "Epoch: 100 of 100, train  Loss: 1.3919 Acc: 0.4700\n",
      "Epoch: 100 of 100, val    Loss: 1.2647 Acc: 0.5700\n",
      "\n",
      "Training complete in 4m 13s\n",
      "Best val Acc: 0.6100 Best val loss: 1.1958\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_conv, best_val_loss, best_val_acc = train_model(data,\n",
    "                                                      model_conv,\n",
    "                                                      criterion,\n",
    "                                                      optimizer_conv,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      num_epochs = 100,\n",
    "                                                      checkpoint = checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3WHcYME2huF"
   },
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model_conv.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_conv.state_dict(),\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'best_val_accuracy': best_val_acc,\n",
    "            'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
    "            }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQuhWHVqVJE_"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ase07EAFAHwO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Happy': 4826, 'Neutral': 3293, 'Sad': 3211, 'Fear': 2613, 'Angry': 2406, 'Surprise': 2057})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "x='train'\n",
    "d = datasets.ImageFolder(os.path.join(DATA_DIR, x))\n",
    "cnt = Counter([])\n",
    "for i,(image,category) in enumerate(d):\n",
    "    cnt.update({(image_datasets['train'].classes)[category]:1})\n",
    "print(cnt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angry'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train'].classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NetWithDataLoader.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "output_auto_scroll": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08e0b965fc614feb960cf954223db3e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e81eb8a8cdb40738fb85ce51cfed6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08e0b965fc614feb960cf954223db3e3",
      "placeholder": "​",
      "style": "IPY_MODEL_6d2c8e3068854041be63a92194b35a64",
      "value": " 44.7M/44.7M [11:04&lt;00:00, 70.5kB/s]"
     }
    },
    "3876f5ef3ff947b7b26c72b6512166f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa2dc4675ff940b48a90425981eb55a6",
       "IPY_MODEL_2e81eb8a8cdb40738fb85ce51cfed6c7"
      ],
      "layout": "IPY_MODEL_79f9ecfbdd9a4871b95ae9668720c847"
     }
    },
    "6d2c8e3068854041be63a92194b35a64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79f9ecfbdd9a4871b95ae9668720c847": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "966b7f8371654dd2a54f484c4a9c0129": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a315579ab89f43e2b7d184c7c4d64baa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa2dc4675ff940b48a90425981eb55a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a315579ab89f43e2b7d184c7c4d64baa",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_966b7f8371654dd2a54f484c4a9c0129",
      "value": 46827520
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
