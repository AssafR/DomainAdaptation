{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "npGiuvftxf1z",
    "outputId": "48a8cb76-f300-4db3-f216-eba77d4cce90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.18</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "import itertools\n",
    "import pixiedust\n",
    "import random\n",
    "from torch.utils import data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "print(torch.__version__)\n",
    "plt.ion()   # interactive mode\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "MHoa2aySwBF_",
    "outputId": "31d7f712-1312-4029-8773-9d4b77d44312"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    LABS_DIR = Path ('/content/gdrive/My Drive/Labs')\n",
    "except:\n",
    "    LABS_DIR = Path ('C:/Labs/')\n",
    "\n",
    "#DATA_DIR = LABS_DIR/'Data'    \n",
    "DATA_DIR = LABS_DIR/'DataNoDuplicates'    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(loader):\n",
    "    cnt = Counter([])\n",
    "    for _,(_,labels) in enumerate(loader):\n",
    "        cnt.update(labels.numpy())\n",
    "    return dict(cnt)\n",
    "\n",
    "def imbalanced_classes_weights(class_counts):\n",
    "    classes = class_counts.keys()\n",
    "    biggest_class = max(class_counts.values())\n",
    "    class_weights = [biggest_class] * (1+max(classes))\n",
    "    for class_no, class_weight in class_counts.items():\n",
    "        class_weights[class_no] = biggest_class/(class_weight+1)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRBBa4e-xRBD"
   },
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# for validatin we use normalization and resize (for train we also change the angle and size of the images)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(10), # Original: 5\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomResizedCrop(224, scale=(0.96, 1.0), ratio=(0.95, 1.05)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.9, 1.1), ratio=(0.9, 1.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "hmElu6Slwjgf",
    "outputId": "6b438b4a-4913-46c6-9bc5-6884fcfa2d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
      "Train classes frequencies:  {6: 1442, 4: 3148, 3: 4627, 5: 3058, 0: 2225, 2: 2344}\n",
      "Train image size: 16844\n",
      "Validation image size: 5296\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "''' The function takes the data loader and a parameter  '''\n",
    "def create_train_val_slice(image_datasets,sample_size=None,val_same_as_train=False):\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    \n",
    "    if not sample_size: # return the whole data set\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n",
    "                                                      shuffle=True, num_workers=1)\n",
    "                      for x in ['train', 'val']}\n",
    "        return dataloaders, dataset_sizes\n",
    "    \n",
    "    sample_n = {x: random.sample(list(range(dataset_sizes[x])), sample_size)\n",
    "                for x in ['train', 'val']}\n",
    "\n",
    "    image_datasets_reduced = {x: torch.utils.data.Subset(image_datasets[x], sample_n[x])\n",
    "                              for x in ['train', 'val']}\n",
    "    \n",
    "    #clone the image_datasets_reduced[train] generator for the val\n",
    "    if val_same_as_train:\n",
    "        image_datasets_reduced['val'] = list(image_datasets_reduced['train'])\n",
    "        image_datasets_reduced['train'] = image_datasets_reduced['val']\n",
    "        \n",
    "    dataset_sizes = {x: len(image_datasets_reduced[x]) for x in ['train', 'val']}\n",
    "\n",
    "    dataloaders_reduced = {x: torch.utils.data.DataLoader(image_datasets_reduced[x], batch_size=BATCH_SIZE,\n",
    "                                                  shuffle=True, num_workers=1) for x in ['train', 'val']}\n",
    "    return dataloaders_reduced, dataset_sizes\n",
    "        \n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "sample_size = 500\n",
    "#data, dataset_sizes =  create_train_val_slice(image_datasets,sample_size,True)\n",
    "data, dataset_sizes =  create_train_val_slice(image_datasets,None)\n",
    "#data, dataset_sizes =  create_train_val_slice(image_datasets,sample_size,val_same_as_train=False) \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_counts = count_classes(data['train'])\n",
    "\n",
    "class_weights = imbalanced_classes_weights(class_counts)\n",
    "\n",
    "\n",
    "print(\"Classes: \", class_names) \n",
    "print(\"Train classes frequencies: \",class_counts)\n",
    "\n",
    "print(f'Train image size: {dataset_sizes[\"train\"]}')\n",
    "print(f'Validation image size: {dataset_sizes[\"val\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "aOMEb5deCofk",
    "outputId": "40524968-328f-420d-cfc1-1e3fe2632a01"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "# # Make a grid from batch\n",
    "# sample_train_images = torchvision.utils.make_grid(inputs)\n",
    "# #imshow(sample_train_images, title=classes)\n",
    "# print(f\"classes={classes}\")\n",
    "# imshow(sample_train_images, title=[class_names[i] for i in classes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     25,
     27,
     79
    ],
    "colab": {},
    "colab_type": "code",
    "id": "sDNvKgitDNG7",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def train_model(data, model, criterion, optimizer, scheduler, num_epochs=2, writer=None, checkpoint = None):\n",
    "    since = time.time()\n",
    "\n",
    "#     if checkpoint is None:\n",
    "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#         best_loss = math.inf\n",
    "#         best_acc = 0.\n",
    "#     else:\n",
    "#         print('Val loss: {}, Val accuracy: {}'\\\n",
    "#               .format(checkpoint[\"best_val_loss\"], checkpoint[\"best_val_accuracy\"]))\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#         best_loss = checkpoint['best_val_loss']\n",
    "#         best_acc = checkpoint['best_val_accuracy']\n",
    "           \n",
    "    print(\"Starting epochs\")\n",
    "    outer = tqdm(total=num_epochs, desc='Epoch', position=0,ncols='100%')\n",
    "    inner = {'train': tqdm(total=dataset_sizes['train']//BATCH_SIZE, desc='Train', position=0,ncols='100%'),\n",
    "             'val': tqdm(total=dataset_sizes['val']//BATCH_SIZE,desc='Val  ', position=0,ncols='100%')}\n",
    "    results = {'train': {'loss':1000.0,'acc':0.0}, 'val': {'loss':1000.0,'acc':0.0}}\n",
    "    #inner_train = tqdm(total=num_epochs, desc='Epoch', position=0,ncols='100%')\n",
    "    \n",
    "    best_loss = -1\n",
    "    best_acc = 0\n",
    "    early_stopping = False\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stopping:\n",
    "            break\n",
    "        outer.update(1)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Handle tqdm inner loop counter\n",
    "            inner[phase].reset()\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(data[phase]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Update inner tqdm, we are about to override the previous maximum, update maximum\n",
    "                inner[phase].update(1) # Advance the tqdm counter\n",
    "                l = results[phase]['loss']\n",
    "                a = results[phase]['acc']\n",
    "                results_phase_desc = f'prev[l,a]=[{l:.4f}/{a:.4f}]'\n",
    "                inner[phase].desc = phase + ' ' + results_phase_desc\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                batch_loss = loss.item() * inputs.size(0)\n",
    "                running_loss += batch_loss\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                if writer is not None and phase=='train':\n",
    "                    x_axis = 1000*(epoch + i/(dataset_sizes[phase]/BATCH_SIZE))\n",
    "                    writer.add_scalar('batch loss',batch_loss/BATCH_SIZE,x_axis)\n",
    "                # print(\"running_corrects =\", running_corrects)\n",
    "                \n",
    "            if phase == 'train' and scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "            #inner.write(\"running_corrects=\", running_corrects, \" epoch: \", epoch)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            results[phase] =  {'loss': epoch_loss,'acc':epoch_acc}\n",
    "            if writer is not None:\n",
    "                x_axis = epoch \n",
    "                writer.add_scalar('accuracy-' + phase,\n",
    "                    epoch_acc,\n",
    "                    x_axis)\n",
    "                \n",
    "            loss_str = f'Epoch: {epoch+1} of {num_epochs}, {phase:6} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}'\n",
    "            \n",
    "            # Early stopping\n",
    "            if phase == 'val' and (results['train']['acc'] > (results['val']['acc'] + 0.2)):\n",
    "                print(f\"Early stopping at epoch {epoch}: train acc={results['train']['acc']} , val_acc={results['val']['acc']}\")\n",
    "                early_stopping = True\n",
    "\n",
    "            \n",
    "\n",
    "#             # deep copy the model\n",
    "#             if phase == 'val' and epoch_loss < best_loss:\n",
    "#                 inner.write('New best model found!')\n",
    "#                 inner.write(f'New record loss:{epoch_loss}, previous record loss: {best_loss}')\n",
    "#                 best_loss = epoch_loss\n",
    "#                 best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#                 #save the weights\n",
    "#                 torch.save(best_model_wts, CHECK_POINT_PATH)\n",
    "        #print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lest val Acc: {:.4f} Last val loss: {:.4f}'.format(results['val']['acc'], results['val']['loss']))\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "3876f5ef3ff947b7b26c72b6512166f8",
      "79f9ecfbdd9a4871b95ae9668720c847",
      "fa2dc4675ff940b48a90425981eb55a6",
      "2e81eb8a8cdb40738fb85ce51cfed6c7",
      "966b7f8371654dd2a54f484c4a9c0129",
      "a315579ab89f43e2b7d184c7c4d64baa",
      "6d2c8e3068854041be63a92194b35a64",
      "08e0b965fc614feb960cf954223db3e3"
     ]
    },
    "colab_type": "code",
    "id": "AUF-cgM1y_7L",
    "outputId": "107315c6-103a-4192-a611-d3e76809d530"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "    #model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "    #model_conv = torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "\n",
    "    # ct = 0\n",
    "    # for child in model_conv.children():\n",
    "    #   ct += 1\n",
    "    #   # freezes layers 1-6 in the total 10 layers of Resnet50\n",
    "    #   if ct < 7:\n",
    "    #     for param in child.parameters():\n",
    "    #       param.requires_grad = False\n",
    "\n",
    "\n",
    "    num_ftrs = model_conv.fc.in_features\n",
    "    model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "    model_conv = model_conv.to(device)\n",
    "    return model_conv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_conv.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35uVilmuNgg-"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UgElP8XDrYm"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''two options to write the loss. They are both equal'''\n",
    "# option 1 #\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
    "\n",
    "# option 2 #\n",
    "# p = nn.functional.softmax(model_conv, dim=1)\n",
    "# # to calculate loss using probabilities you can do below \n",
    "# criterion = nn.functional.nll_loss(torch.log(p), y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IaCCoou62TKD",
    "outputId": "058f59dd-7db0-4456-f29d-42ae2d4567c6"
   },
   "outputs": [],
   "source": [
    "CHECK_POINT_PATH = LABS_DIR/'ModelParams'/'checkpoint.tar'\n",
    "\n",
    "# !del $CHECK_POINT_PATH\n",
    "\n",
    "# try:\n",
    "#     checkpoint = torch.load(CHECK_POINT_PATH)\n",
    "#     print(\"checkpoint loaded\")\n",
    "# except:\n",
    "#     checkpoint = None\n",
    "#     print(\"checkpoint not found\")\n",
    "checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_tKaq9vU0cjq",
    "outputId": "1b3c5122-2b8f-4c1c-cedc-811cbfe8fcfe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97491c72643743f79a940348f2150658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', layout=Layout(flex='2'), max=50.0, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f6b04acbc84af4bb22ec1ebd275a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', layout=Layout(flex='2'), max=526.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6edc5d31d8d4634958c318997f48562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Val  ', layout=Layout(flex='2'), max=165.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 67m 9s\n",
      "Lest val Acc: 0.6299 Last val loss: 3.4033\n",
      "Starting epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a387f92b47e4d23ad6851c444fcaa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', layout=Layout(flex='2'), max=50.0, style=Progress…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c80251f62a24857ad510519c3e00f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train', layout=Layout(flex='2'), max=526.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a9ca7d454a424ca6af58967da22354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Val  ', layout=Layout(flex='2'), max=165.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 66m 43s\n",
      "Lest val Acc: 0.5476 Last val loss: 3.6881\n",
      "Wall time: 2h 13min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tensorboard Stuff\n",
    "#lr_initial = 0.1\n",
    "momentum = 0.9\n",
    "scheduler_step_size = 7\n",
    "scheduler_gamma = 0.3\n",
    "\n",
    "def run_experiment(model, lr_initial, momentum, scheduler_gamma, scheduler_step_size):\n",
    "    # Observe that only parameters of final layer are being optimized\n",
    "    #optimizer_conv = optim.SGD(model.fc.parameters(), lr=lr_initial, momentum=momentum)\n",
    "    \n",
    "    weight_decay = 0.001\n",
    "    \n",
    "    optimizer_conv = optim.Adam(model.parameters(),lr=lr_initial,weight_decay=weight_decay)\n",
    "    \n",
    "    # e.g. Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=scheduler_step_size, gamma=scheduler_gamma)\n",
    "#     exp_lr_scheduler = None\n",
    "#     experiment_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") +\\\n",
    "#         f'_lr_{lr_initial}_mmt_{momentum}_st_{scheduler_step_size}_gma_{scheduler_gamma}'\n",
    "    experiment_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") +\\\n",
    "        f'_ADAM_Weights_wdecay_{weight_decay}_imb_Scale01_sched_{scheduler_step_size}_gam_{scheduler_gamma}_lr_{lr_initial}'\n",
    "    writer = SummaryWriter('runs/' + experiment_name)\n",
    "    model_conv, best_val_loss, best_val_acc = train_model(data,\n",
    "                                                          model,\n",
    "                                                          criterion,\n",
    "                                                          optimizer_conv,\n",
    "                                                          exp_lr_scheduler,\n",
    "                                                          num_epochs=50,\n",
    "                                                          writer=writer,\n",
    "                                                          checkpoint=checkpoint)\n",
    "    return model_conv, best_val_loss, best_val_acc \n",
    "\n",
    "for lr in [0.001,0.005]: #[0.1,0.05,0.01]:\n",
    "    for scheduler_step_size in [7]:\n",
    "        for scheduler_gamma in [0.5]: #[0.1,0.2,0.3,0.4]:\n",
    "            model = get_model()\n",
    "            model_conv, best_val_loss, best_val_acc  = \\\n",
    "                run_experiment(model, lr, momentum, scheduler_gamma, scheduler_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C3WHcYME2huF"
   },
   "outputs": [],
   "source": [
    "# torch.save({'model_state_dict': model_conv.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer_conv.state_dict(),\n",
    "#             'best_val_loss': best_val_loss,\n",
    "#             'best_val_accuracy': best_val_acc,\n",
    "#             'scheduler_state_dict' : exp_lr_scheduler.state_dict(),\n",
    "#             }, CHECK_POINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQuhWHVqVJE_"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ase07EAFAHwO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Happy': 4627, 'Neutral': 3148, 'Sad': 3058, 'Fear': 2344, 'Angry': 2225, 'Surprise': 1442})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "x='train'\n",
    "d = datasets.ImageFolder(os.path.join(DATA_DIR, x))\n",
    "cnt = Counter([])\n",
    "for i,(image,category) in enumerate(d):\n",
    "    cnt.update({(image_datasets['train'].classes)[category]:1})\n",
    "print(cnt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Angry'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train'].classes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 4627, 4: 3148, 5: 3058, 2: 2344, 0: 2225, 6: 1442})\n",
      "4627\n"
     ]
    }
   ],
   "source": [
    "import torch as pt\n",
    "cnt = Counter([])\n",
    "for i,(inputs,labels) in enumerate(data['train']):\n",
    "#     if i>5:\n",
    "#         break;\n",
    "    cnt.update(labels.numpy())\n",
    "print(cnt)\n",
    "print(cnt[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NetWithDataLoader.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "output_auto_scroll": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08e0b965fc614feb960cf954223db3e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e81eb8a8cdb40738fb85ce51cfed6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08e0b965fc614feb960cf954223db3e3",
      "placeholder": "​",
      "style": "IPY_MODEL_6d2c8e3068854041be63a92194b35a64",
      "value": " 44.7M/44.7M [11:04&lt;00:00, 70.5kB/s]"
     }
    },
    "3876f5ef3ff947b7b26c72b6512166f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa2dc4675ff940b48a90425981eb55a6",
       "IPY_MODEL_2e81eb8a8cdb40738fb85ce51cfed6c7"
      ],
      "layout": "IPY_MODEL_79f9ecfbdd9a4871b95ae9668720c847"
     }
    },
    "6d2c8e3068854041be63a92194b35a64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79f9ecfbdd9a4871b95ae9668720c847": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "966b7f8371654dd2a54f484c4a9c0129": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a315579ab89f43e2b7d184c7c4d64baa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa2dc4675ff940b48a90425981eb55a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a315579ab89f43e2b7d184c7c4d64baa",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_966b7f8371654dd2a54f484c4a9c0129",
      "value": 46827520
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
